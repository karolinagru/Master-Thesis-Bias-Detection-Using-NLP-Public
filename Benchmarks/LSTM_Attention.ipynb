{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import re\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense,multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '../Datasets/'\n",
    "train_df = pd.read_csv(prefix + 'train_for_db.tsv', sep='\\t')\n",
    "test_df = pd.read_csv(prefix + 'test_for_db.tsv', sep='\\t')\n",
    "eval_df = pd.read_csv(prefix + 'val_for_db.tsv', sep='\\t')\n",
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "eval_df = eval_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(625410,)\n",
      "(78176,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(train_df[\"text\"])\n",
    "Y_train = np.array(train_df[\"feedback_realtor\"]).reshape((-1, 1))\n",
    "X_test = np.array(eval_df[\"text\"])\n",
    "Y_test = np.array(eval_df[\"feedback_realtor\"]).reshape((-1, 1))\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print(\"Running on TPU \", tpu.cluster_spec().as_dict()[\"worker\"])\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Vectors\n",
    "MAX_FEATURES = 70000\n",
    "EMBED_DIM = 300\n",
    "\n",
    "# LSTM\n",
    "LSTM_OUTPUT_SIZE = 200\n",
    "MAXLEN = 200\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 256* strategy.num_replicas_in_sync\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max vector length: 200\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(lower=True, split=\" \", num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_vec = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_vec = tokenizer.texts_to_sequences(X_test)\n",
    "#max([len(x) for x in X_train_vec])\n",
    "print(f\"Max vector length: {MAXLEN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad with zeros for same vector length\n",
    "X_train_vec = sequence.pad_sequences(X_train_vec, maxlen=MAXLEN, padding=\"post\")\n",
    "X_test_vec = sequence.pad_sequences(X_test_vec, maxlen=MAXLEN, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max vector length: 200\n"
     ]
    }
   ],
   "source": [
    "X_test_final = np.array(test_df[\"text\"])\n",
    "Y_test_final = np.array(test_df[\"feedback_realtor\"]).reshape((-1, 1))\n",
    "\n",
    "tokenizer = Tokenizer(lower=True, split=\" \", num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(X_test_final)\n",
    "X_test_final_vec = tokenizer.texts_to_sequences(X_test_final)\n",
    "MAXLEN = 200\n",
    "#max([len(x) for x in X_train_vec])\n",
    "print(f\"Max vector length: {MAXLEN}\")\n",
    "X_test_final = sequence.pad_sequences(X_test_final_vec, maxlen=MAXLEN, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = './outputs_new/lstm_attention'\n",
    "PATH_MODELS = PATH_DATA + \"model_pre_trained/\"\n",
    "PATH_CHECKPOINTS = PATH_MODELS + \"checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_adam = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 200, 300)          21000000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 200, 200)          400800    \n",
      "_________________________________________________________________\n",
      "attention (Attention)        (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 201       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 21,401,403\n",
      "Trainable params: 21,401,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define NN architecture\n",
    "embedding_layer = Embedding(MAX_FEATURES,\n",
    "        EMBED_DIM,\n",
    "        input_length = 200)\n",
    "\n",
    "lstm_layer = LSTM(LSTM_OUTPUT_SIZE, dropout=0.1, recurrent_dropout=0.1,return_sequences=True)\n",
    "text_input = Input(shape=(200,), dtype='int32')\n",
    "embedded_sequences= embedding_layer(text_input)\n",
    "x = lstm_layer(embedded_sequences)\n",
    "merged = Attention(200)(x)\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "model = Model(inputs=[text_input], \\\n",
    "        outputs=preds)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer=opt_adam,\n",
    "        metrics=['accuracy', 'AUC'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'num_layers': self.num_layers,\n",
    "            'units': self.units,\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dropout': self.dropout,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Stop training when validation acc starts dropping\n",
    "# Save checkpoint of model each period\n",
    "now = datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
    "# Create callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", verbose=1, patience=2),\n",
    "    ModelCheckpoint(\n",
    "        PATH_CHECKPOINTS + now + \"_Model_BILSTM-Embed_.h5\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Fit the model\n",
    "steps_per_epoch = int(np.floor((len(X_train_vec) / BATCH_SIZE)))\n",
    "print(\n",
    "    f\"Model Params.\\nbatch_size: {BATCH_SIZE}\\nEpochs: {EPOCHS}\\n\"\n",
    "    f\"Step p. Epoch: {steps_per_epoch}\\n\"\n",
    ")\n",
    "\n",
    "hist = model.fit(\n",
    "    X_train_vec,\n",
    "    Y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_test_vec, Y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_lstm_attention_150721/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('model_lstm_attention_150721')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "prediction = model.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.to_csv('prediction_lstm_attention.csv', sep = ',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
