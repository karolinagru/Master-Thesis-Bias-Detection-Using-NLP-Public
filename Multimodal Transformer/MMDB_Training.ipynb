{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal German DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install tensorboard\n",
    "!pip install keras tensorflow\n",
    "!pip install transformers == 3.1.0\n",
    "!pip install multimodal-transformers\n",
    "!pip install --no-cache-dir tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "from multimodal_transformers.data import load_data\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from multimodal_transformers.model import AutoModelWithTabular, TabularConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    Trainer,\n",
    "    EvalPrediction,\n",
    "    set_seed\n",
    ")\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "from multimodal_transformers.data import load_data_from_folder\n",
    "from multimodal_transformers.model import TabularConfig\n",
    "from multimodal_transformers.model import AutoModelWithTabular\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "os.environ['COMET_MODE'] = 'DISABLED'\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.is_available()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_given = pd.read_csv('df_given_labels_200k.tsv',\n",
    "                    sep='\\t')\n",
    "df_predicted  = pd.read_csv('test_overwritten_labels_600k.tsv',\n",
    "                    sep='\\t')\n",
    "df_predicted = df_predicted[[\"text\",\"income_group\",\"employment_group\",\"marital_status\",\"nationality\",\"age_decade\",\n",
    "                            \"avg_res_realtor\",\"feedback_realtor\"]]\n",
    "df_predicted= df_predicted[df_predicted['text'] != \"None\"]\n",
    "df = pd.concat([df_given, df_predicted], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "# Assigning numerical values and storing in another column\n",
    "df['feedback_realtor'] = labelencoder.fit_transform(df['feedback_realtor'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['feedback_realtor'] = pd.Categorical(df.feedback_realtor)\n",
    "df['nationality'] = pd.Categorical(df.nationality)\n",
    "df['marital_status'] = pd.Categorical(df.marital_status)\n",
    "df['income_group'] = pd.Categorical(df.income_group)\n",
    "df['employment_group'] = pd.Categorical(df.employment_group)\n",
    "df['age_decade'] = pd.Categorical(df.age_decade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = np.split(df.sample(frac=1), [int(.8*len(df)), int(.9 * len(df))])\n",
    "print('Num examples train-val-test')\n",
    "print(len(train_df), len(val_df), len(test_df))\n",
    "train_df.to_csv('./Datasets/train.csv')\n",
    "val_df.to_csv('./Datasets/val.csv')\n",
    "test_df.to_csv('./Datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "  \"\"\"\n",
    "  Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "  \"\"\"\n",
    "\n",
    "  model_name_or_path: str = field(\n",
    "      metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "  )\n",
    "  config_name: Optional[str] = field(\n",
    "      default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "  )\n",
    "  tokenizer_name: Optional[str] = field(\n",
    "      default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "  )\n",
    "  cache_dir: Optional[str] = field(\n",
    "      default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MultimodalDataTrainingArguments:\n",
    "  \"\"\"\n",
    "  Arguments pertaining to how we combine tabular features\n",
    "  Using `HfArgumentParser` we can turn this class\n",
    "  into argparse arguments to be able to specify them on\n",
    "  the command line.\n",
    "  \"\"\"\n",
    "\n",
    "  data_path: str = field(metadata={\n",
    "                            'help': 'the path to the csv file containing the dataset'\n",
    "                        })\n",
    "  column_info_path: str = field(\n",
    "      default=None,\n",
    "      metadata={\n",
    "          'help': 'the path to the json file detailing which columns are text, categorical, numerical, and the label'\n",
    "  })\n",
    "\n",
    "  column_info: dict = field(\n",
    "      default=None,\n",
    "      metadata={\n",
    "          'help': 'a dict referencing the text, categorical, numerical, and label columns'\n",
    "                  'its keys are text_cols, num_cols, cat_cols, and label_col'\n",
    "  })\n",
    "\n",
    "  categorical_encode_type: str = field(default='ohe',\n",
    "                                        metadata={\n",
    "                                            'help': 'sklearn encoder to use for categorical data',\n",
    "                                            'choices': ['ohe', 'binary', 'label', 'none']\n",
    "                                        })\n",
    "  numerical_transformer_method: str = field(default='yeo_johnson',\n",
    "                                            metadata={\n",
    "                                                'help': 'sklearn numerical transformer to preprocess numerical data',\n",
    "                                                'choices': ['yeo_johnson', 'box_cox', 'quantile_normal', 'none']\n",
    "                                            })\n",
    "  task: str = field(default=\"classification\",\n",
    "                    metadata={\n",
    "                        \"help\": \"The downstream training task\",\n",
    "                        \"choices\": [\"classification\", \"regression\"]\n",
    "                    })\n",
    "\n",
    "  mlp_division: int = field(default=4,\n",
    "                            metadata={\n",
    "                                'help': 'the ratio of the number of '\n",
    "                                        'hidden dims in a current layer to the next MLP layer'\n",
    "                            })\n",
    "  combine_feat_method: str = field(default='individual_mlps_on_cat_and_numerical_feats_then_concat',\n",
    "                                    metadata={\n",
    "                                        'help': 'method to combine categorical and numerical features, '\n",
    "                                                'see README for all the method'\n",
    "                                    })\n",
    "  mlp_dropout: float = field(default=0.1,\n",
    "                              metadata={\n",
    "                                'help': 'dropout ratio used for MLP layers'\n",
    "                              })\n",
    "  numerical_bn: bool = field(default=True,\n",
    "                              metadata={\n",
    "                                  'help': 'whether to use batchnorm on numerical features'\n",
    "                              })\n",
    "  use_simple_classifier: str = field(default=True,\n",
    "                                      metadata={\n",
    "                                          'help': 'whether to use single layer or MLP as final classifier'\n",
    "                                      })\n",
    "  mlp_act: str = field(default='relu',\n",
    "                        metadata={\n",
    "                            'help': 'the activation function to use for finetuning layers',\n",
    "                            'choices': ['relu', 'prelu', 'sigmoid', 'tanh', 'linear']\n",
    "                        })\n",
    "  gating_beta: float = field(default=0.2,\n",
    "                              metadata={\n",
    "                                  'help': \"the beta hyperparameters used for gating tabular data \"\n",
    "                                          \"see https://www.aclweb.org/anthology/2020.acl-main.214.pdf\"\n",
    "                              })\n",
    "\n",
    "  def __post_init__(self):\n",
    "      assert self.column_info != self.column_info_path\n",
    "      if self.column_info is None and self.column_info_path:\n",
    "          with open(self.column_info_path, 'r') as f:\n",
    "              self.column_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = ['text']\n",
    "cat_cols = ['nationality', 'marital_status', 'income_group','employment_group','age_decade']\n",
    "num_cols = ['avg_res_realtor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_info_dict = {\n",
    "    'text_cols': text_cols,\n",
    "    'cat_cols': cat_cols,\n",
    "    'num_cols' : num_cols,\n",
    "    'label_col': 'feedback_realtor',\n",
    "    'label_list': ['0','1']\n",
    "}\n",
    "\n",
    "data_args = MultimodalDataTrainingArguments(\n",
    "    data_path='./Datasets/',\n",
    "    combine_feat_method='weighted_feature_sum_on_transformer_cat_and_numerical_feats',\n",
    "    column_info=column_info_dict,\n",
    "    task='classification'\n",
    ")\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path='distilbert-base-german-cased'\n",
    ")\n",
    "\n",
    "\n",
    "data_args = MultimodalDataTrainingArguments(\n",
    "    data_path='./Datasets/',\n",
    "    combine_feat_method='weighted_feature_sum_on_transformer_cat_and_numerical_feats',\n",
    "    column_info=column_info_dict,\n",
    "    task='classification'\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./logs/model_name/mmbt_2epoch_64/\",\n",
    "    logging_dir=\"./logs/runs/mmbt_2epoch_64/\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=64,\n",
    "    num_train_epochs=2,\n",
    "    evaluate_during_training=True,\n",
    "    logging_steps= 10000,\n",
    "    save_steps=15000,\n",
    "    eval_steps=104235,\n",
    "    save_total_limit = 4\n",
    ")\n",
    "\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specified tokenizer:  distilbert-base-german-cased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140092409603520 acquired on /home/ec2-user/.cache/torch/transformers/91dc05ca5527bfe4665fbe48c61a27f70a2f95564557a0553110430301b6a4e3.9e9be6c7ed31b75b3c39946d76205fe34192bbd3b7a97e205ebf28fa13825de2.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c1042b48634f3c8db2528392b8dd6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/464 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140092409603520 released on /home/ec2-user/.cache/torch/transformers/91dc05ca5527bfe4665fbe48c61a27f70a2f95564557a0553110430301b6a4e3.9e9be6c7ed31b75b3c39946d76205fe34192bbd3b7a97e205ebf28fa13825de2.lock\n",
      "INFO:filelock:Lock 140095794614224 acquired on /home/ec2-user/.cache/torch/transformers/221b1368d82380622e22df082e9bfdafece5b7d5c42a0cd73b57d89247d2f28d.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1847fde9885b40b08784f4c561c55b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/240k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140095794614224 released on /home/ec2-user/.cache/torch/transformers/221b1368d82380622e22df082e9bfdafece5b7d5c42a0cd73b57d89247d2f28d.bac90776f6fa34759f05c7387e9124c4d626300e981b5786b82f674e08f99d72.lock\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path_or_name = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path\n",
    "print('Specified tokenizer: ', tokenizer_path_or_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_path_or_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:multimodal_transformers.data.data_utils:1 numerical columns\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "INFO:multimodal_transformers.data.data_utils:35 categorical columns\n",
      "INFO:multimodal_transformers.data.data_utils:1 numerical columns\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "INFO:multimodal_transformers.data.load_data:Text columns: ['text']\n",
      "INFO:multimodal_transformers.data.load_data:Raw text example: Sehr geehrter Herr Römer , Mein derzeitiger Mitbewohner m 24 und ich m 23 wohnen zur Zeit in Düsseldorf . Arbeitsbedingt zieht es uns nun aber beide nach Köln . Wir haben beide einen unbefristeten Arbeitsvertrag . Er in der Versicherungsbranche und ich als Garten Landschaftsarchitekt . Wir beabsichtigen einen Einzug ab dem 01 . 10 . 2019 . Die Lage Ihrer Immobilie wäre ideal für uns ! Dadurch dass wir beide Vollzeit arbeiten und am Wochenende in Belgien wohnen , können wir mit gutem Gewissen behaupten , dass wir ruhige Mieter sind . Gerne würden wir einen Besichtigungstermin vereinbaren . Vielen Dank im Voraus ! Mit freundlichen Grüßen Ron Firmenich\n",
      "INFO:multimodal_transformers.data.data_utils:35 categorical columns\n",
      "INFO:multimodal_transformers.data.data_utils:1 numerical columns\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "INFO:multimodal_transformers.data.load_data:Text columns: ['text']\n",
      "INFO:multimodal_transformers.data.load_data:Raw text example: Sehr geehrte Frau Langefeld , ich interessiere mich für die Wohnung und würde sie sehr gerne besichtigen . Mit freundlichem Gruß Susann Witt 040 87 60 70 38\n",
      "INFO:multimodal_transformers.data.data_utils:35 categorical columns\n",
      "INFO:multimodal_transformers.data.data_utils:1 numerical columns\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n",
      "INFO:multimodal_transformers.data.load_data:Text columns: ['text']\n",
      "INFO:multimodal_transformers.data.load_data:Raw text example: Sehr geehrter Damen und Herren , hiermit bewerbe ich mich für einen Besichtigungstermin . Bin angestellte , habe 3 Kinder . Vielen Dank mit freundlichen grüßen Swetlana Ottinger\n"
     ]
    }
   ],
   "source": [
    "# Get Datasets\n",
    "train_dataset, val_dataset, test_dataset = load_data_from_folder(\n",
    "    data_args.data_path,\n",
    "    data_args.column_info['text_cols'],\n",
    "    tokenizer,\n",
    "    label_col=data_args.column_info['label_col'],\n",
    "    label_list=data_args.column_info['label_list'],\n",
    "    categorical_cols=data_args.column_info['cat_cols'],\n",
    "    numerical_cols = data_args.column_info['num_cols'],\n",
    "    numerical_transformer_method = 'quantile_normal',\n",
    "    max_token_length = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "tabular_config = TabularConfig(num_labels=num_labels,\n",
    "                               cat_feat_dim=train_dataset.cat_feats.shape[1],\n",
    "                               numerical_feat_dim=train_dataset.numerical_feats.shape[1],\n",
    "                               **vars(data_args))\n",
    "config.tabular_config = tabular_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140092292705472 acquired on /home/ec2-user/.cache/torch/transformers/5d61179239b4209e055881d93e51e5a69f1901f76f964d7bc92c9a62a7fc5026.59e237bd040884abec825b1267b5b13223e1400bfcef5a4a3fee71c4fc21a989.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8373d6b9c76d4d73a028a7a8329bfe79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/270M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140092292705472 released on /home/ec2-user/.cache/torch/transformers/5d61179239b4209e055881d93e51e5a69f1901f76f964d7bc92c9a62a7fc5026.59e237bd040884abec825b1267b5b13223e1400bfcef5a4a3fee71c4fc21a989.lock\n",
      "Some weights of the model checkpoint at distilbert-base-german-cased were not used when initializing DistilBertWithTabular: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertWithTabular from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertWithTabular from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertWithTabular were not initialized from the model checkpoint at distilbert-base-german-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'tabular_combiner.weight_cat', 'tabular_combiner.weight_num', 'tabular_combiner.num_bn.weight', 'tabular_combiner.num_bn.bias', 'tabular_combiner.num_bn.running_mean', 'tabular_combiner.num_bn.running_var', 'tabular_combiner.cat_layer.weight', 'tabular_combiner.cat_layer.bias', 'tabular_combiner.num_layer.weight', 'tabular_combiner.num_layer.bias', 'tabular_combiner.layer_norm.weight', 'tabular_combiner.layer_norm.bias', 'tabular_classifier.weight', 'tabular_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelWithTabular.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_classification_metrics(p: EvalPrediction):\n",
    "  pred_labels = np.argmax(p.predictions, axis=1)\n",
    "  pred_scores = softmax(p.predictions, axis=1)[:, 1]\n",
    "  labels = p.label_ids\n",
    "  if len(np.unique(labels)) == 2:  # binary classification\n",
    "      roc_auc_pred_score = roc_auc_score(labels, pred_scores)\n",
    "      precisions, recalls, thresholds = precision_recall_curve(labels,\n",
    "                                                                pred_scores)\n",
    "      fscore = (2 * precisions * recalls) / (precisions + recalls)\n",
    "      fscore[np.isnan(fscore)] = 0\n",
    "      ix = np.argmax(fscore)\n",
    "      threshold = thresholds[ix].item()\n",
    "      pr_auc = auc(recalls, precisions)\n",
    "      tn, fp, fn, tp = confusion_matrix(labels, pred_labels, labels=[0, 1]).ravel()\n",
    "      result = {'roc_auc': roc_auc_pred_score,\n",
    "                'threshold': threshold,\n",
    "                'pr_auc': pr_auc,\n",
    "                'recall': recalls[ix].item(),\n",
    "                'precision': precisions[ix].item(), 'f1': fscore[ix].item(),\n",
    "                'tn': tn.item(), 'fp': fp.item(), 'fn': fn.item(), 'tp': tp.item()\n",
    "                }\n",
    "  else:\n",
    "      acc = (pred_labels == labels).mean()\n",
    "      f1 = f1_score(y_true=labels, y_pred=pred_labels)\n",
    "      result = {\n",
    "          \"acc\": acc,\n",
    "          \"f1\": f1,\n",
    "          \"acc_and_f1\": (acc + f1) / 2,\n",
    "          \"mcc\": matthews_corrcoef(labels, pred_labels)\n",
    "      }\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_distilbert = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=calc_classification_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e377798cd6a24c25b6b0f4816425dd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff5a1bdeea24412a94d5ea54abfd2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/9773 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a18bd58f0446c79c944083a4874cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/9773 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.61130146484375, 'learning_rate': 2.4419318530645658e-05, 'epoch': 1.0232272587741738, 'step': 10000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7h 37min 40s, sys: 2h 17min 5s, total: 9h 54min 46s\n",
      "Wall time: 9h 54min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=19546, training_loss=0.593411355744142)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer_distilbert.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.configuration_utils import PretrainedConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./logs/model_name/saved_model_2epochs_64batches/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
